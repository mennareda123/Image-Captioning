{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98f82a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\20114\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\20114\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# الخلية 1: تحميل المكتبات المطلوبة\n",
    "# هذه المكتبات تستخدم لمعالجة النصوص، معالجة الصور، بناء الشبكات العصبية، والتدريب\n",
    "import nltk\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import Counter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.datasets import CocoCaptions\n",
    "from torchvision import transforms, models\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "import torch.optim as optim\n",
    "\n",
    "# تحميل بيانات NLTK اللازمة لتقطيع النصوص\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90e01622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# الخلية 2: إعداد تحويلات الصور ومسارات البيانات\n",
    "# هذه التحويلات تعد الصور للشبكة العصبية (تغيير الحجم وتحويلها إلى تنسيق tensor)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# مسارات بيانات COCO\n",
    "root = r\"D:\\Level_3\\term 1\\Deep_Learning\\Codes\\Fenv\\subset_coco\\images\\train2017\"\n",
    "annFile = r\"subset_coco/annotations/captions_train2017_40k.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=1.40s)\n",
      "creating index...\n",
      "index created!\n",
      "Number of samples: 40000\n",
      "Image shape: torch.Size([3, 224, 224])\n",
      "Number of captions: 5\n",
      "First caption: A piece of cake and coffee are on an outdoor table.\n"
     ]
    }
   ],
   "source": [
    "# الخلية 3: تحميل بيانات COCO وفحصها\n",
    "# هنا نقوم بتحميل مجموعة بيانات COCO للصور والتعليقات النصية المصاحبة\n",
    "coco_train = CocoCaptions(\n",
    "    root=root,\n",
    "    annFile=annFile,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "print(\"Number of samples:\", len(coco_train))\n",
    "\n",
    "# اختبار تحميل عينة لفحص شكل البيانات\n",
    "img, captions = coco_train[5]\n",
    "print(f\"Image shape: {img.shape}\")\n",
    "print(f\"Number of captions: {len(captions)}\")\n",
    "print(f\"First caption: {captions[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.80s)\n",
      "creating index...\n",
      "index created!\n",
      "Number of samples: 40000\n",
      "Image shape: torch.Size([3, 224, 224])\n",
      "Number of captions: 5\n",
      "First caption: A piece of cake and coffee are on an outdoor table.\n"
     ]
    }
   ],
   "source": [
    "# الخلية 3: تحميل بيانات COCO وفحصها\n",
    "# هنا نقوم بتحميل مجموعة بيانات COCO للصور والتعليقات النصية المصاحبة\n",
    "coco_train = CocoCaptions(\n",
    "    root=root,\n",
    "    annFile=annFile,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "print(\"Number of samples:\", len(coco_train))\n",
    "\n",
    "# اختبار تحميل عينة لفحص شكل البيانات\n",
    "img, captions = coco_train[5]\n",
    "print(f\"Image shape: {img.shape}\")\n",
    "print(f\"Number of captions: {len(captions)}\")\n",
    "print(f\"First caption: {captions[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# الخلية 4: وظيفة تنظيف النصوص\n",
    "# تقوم بتنظيف النص من الرموز الخاصة وتحويله إلى حروف صغيرة ثم تقسيمه إلى كلمات\n",
    "def clean_caption(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]+\", \" \", text)\n",
    "    return nltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# الخلية 5: بناء المفردات (Vocabulary)\n",
    "# هذه الفئة تبني مفردات من الكلمات المستخدمة في التعليقات مع تجاهل الكلمات النادرة\n",
    "class Vocabulary:\n",
    "    def __init__(self, freq_threshold=5):\n",
    "        self.freq_threshold = freq_threshold\n",
    "        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n",
    "        self.stoi = {v: k for k, v in self.itos.items()}\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        text = text.lower()\n",
    "        text = re.sub(r\"[^a-zA-Z0-9]+\", \" \", text)\n",
    "        return nltk.word_tokenize(text)\n",
    "\n",
    "    def build(self, captions):\n",
    "        counter = Counter()\n",
    "        for c in captions:\n",
    "            counter.update(self.tokenize(c))\n",
    "\n",
    "        idx = 4\n",
    "        for word, freq in counter.items():\n",
    "            if freq >= self.freq_threshold:\n",
    "                self.stoi[word] = idx\n",
    "                self.itos[idx] = word\n",
    "                idx += 1\n",
    "\n",
    "    def numericalize(self, text):\n",
    "        tokens = self.tokenize(text)\n",
    "        return [self.stoi.get(t, self.stoi[\"<UNK>\"]) for t in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 6431\n"
     ]
    }
   ],
   "source": [
    "# الخلية 6: بناء المفردات من جميع التعليقات\n",
    "# هنا نقوم بجمع كل التعليقات من مجموعة البيانات وبناء المفردات\n",
    "all_caps = []\n",
    "\n",
    "for i in range(len(coco_train)):\n",
    "    _, caps = coco_train[i]\n",
    "    all_caps.extend(caps)\n",
    "\n",
    "vocab = Vocabulary(freq_threshold=5)\n",
    "vocab.build(all_caps)\n",
    "\n",
    "print(\"Vocabulary size:\", len(vocab.itos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1663bb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# الخلية 7: فئة Dataset المخصصة لبيانات COCO\n",
    "# تحول الصور والتعليقات إلى تنسيق يمكن للشبكة العصبية التعامل معه\n",
    "class CocoDS(Dataset):\n",
    "    def __init__(self, root, annFile, transform, vocab):\n",
    "        self.ds = CocoCaptions(root, annFile, transform)\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, caps = self.ds[idx]\n",
    "        cap = caps[0]  # نأخذ أول تعليق فقط\n",
    "\n",
    "        numerical = [self.vocab.stoi[\"<SOS>\"]]\n",
    "        numerical += self.vocab.numericalize(cap)\n",
    "        numerical += [self.vocab.stoi[\"<EOS>\"]]\n",
    "\n",
    "        return img, torch.tensor(numerical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# الخلية 8: وظيفة collate_fn لتنظيم الدفعات\n",
    "# تقوم بترتيب البيانات في الدفعات حسب طول النصوص وإضافة حشوات (padding)\n",
    "def collate_fn(batch):\n",
    "    batch.sort(key=lambda x: len(x[1]), reverse=True)\n",
    "    imgs, caps = zip(*batch)\n",
    "\n",
    "    lengths = torch.tensor([len(cap) for cap in caps])\n",
    "    imgs = torch.stack(imgs)\n",
    "\n",
    "    caps = torch.nn.utils.rnn.pad_sequence(\n",
    "        caps,\n",
    "        batch_first=True,\n",
    "        padding_value=vocab.stoi[\"<PAD>\"]\n",
    "    )\n",
    "\n",
    "    return imgs, caps, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.68s)\n",
      "creating index...\n",
      "index created!\n",
      "DataLoader جاهز!\n"
     ]
    }
   ],
   "source": [
    "# الخلية 9: إنشاء DataLoader\n",
    "# يقوم بتحميل البيانات على شكل دفعات للتدريب\n",
    "dataset = CocoDS(root, annFile, transform, vocab)\n",
    "\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=20,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "print(\"DataLoader جاهز!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# الخلية 10: تحديد الجهاز (GPU إذا متوفر)\n",
    "# اختيار بين GPU أو CPU للتدريب الأسرع\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# الخلية 11: بناء نموذج التشفير (Encoder) CNN\n",
    "# يستخدم ResNet50 لاستخراج المميزات من الصور\n",
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        resnet = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
    "        modules = list(resnet.children())[:-2]\n",
    "        self.backbone = nn.Sequential(*modules)\n",
    "        self.avgpool = resnet.avgpool\n",
    "        self.embed = nn.Linear(resnet.fc.in_features, embed_size)\n",
    "        self.bn = nn.BatchNorm1d(embed_size, momentum=0.01)\n",
    "    \n",
    "    def forward(self, images):\n",
    "        features_map = self.backbone(images)\n",
    "        b, c, h, w = features_map.size()\n",
    "        features_seq = features_map.permute(0, 2, 3, 1).contiguous().view(b, h*w, c)\n",
    "        pooled = self.avgpool(features_map)\n",
    "        pooled = pooled.view(pooled.size(0), -1)\n",
    "        pooled = self.bn(self.embed(pooled))\n",
    "        return pooled, features_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9db794d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# الخلية 12: بناء نماذج فك التشفير المختلفة (Decoder)\n",
    "# هنا نجد ثلاثة أنواع من فك التشفير: أساسي، مع انتباه (attention)، ومحول (transformer)\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "    \n",
    "    def forward(self, features, captions, lengths):\n",
    "        embeddings = self.embed(captions)\n",
    "        inputs = torch.cat((features.unsqueeze(1), embeddings), 1)\n",
    "        packed = pack_padded_sequence(inputs, lengths.cpu(), batch_first=True)\n",
    "        hiddens, _ = self.lstm(packed)\n",
    "        outputs = self.linear(hiddens[0])\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, encoder_dim, decoder_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.encoder_att = nn.Linear(encoder_dim, decoder_dim)\n",
    "        self.decoder_att = nn.Linear(decoder_dim, decoder_dim)\n",
    "        self.full_att = nn.Linear(decoder_dim, 1)\n",
    "    \n",
    "    def forward(self, encoder_outputs, decoder_hidden):\n",
    "        att1 = self.encoder_att(encoder_outputs)\n",
    "        att2 = self.decoder_att(decoder_hidden).unsqueeze(1)\n",
    "        energy = self.full_att(torch.tanh(att1 + att2)).squeeze(2)\n",
    "        alpha = F.softmax(energy, dim=1)\n",
    "        attention_weighted_encoding = (encoder_outputs * alpha.unsqueeze(2)).sum(dim=1)\n",
    "        return attention_weighted_encoding, alpha\n",
    "\n",
    "\n",
    "class DecoderWithLSTM(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, encoder_dim=2048, num_layers=1):\n",
    "        super(DecoderWithLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.attention = Attention(encoder_dim, hidden_size)\n",
    "        self.lstm = nn.LSTM(embed_size + encoder_dim, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "    \n",
    "    def forward(self, encoder_outputs, captions):\n",
    "        embeddings = self.embedding(captions)\n",
    "        h = torch.zeros(1, captions.size(0), self.lstm.hidden_size).to(captions.device)\n",
    "        c = torch.zeros(1, captions.size(0), self.lstm.hidden_size).to(captions.device)\n",
    "        outputs = []\n",
    "        \n",
    "        for t in range(captions.size(1)):\n",
    "            attention_weighted_encoding, _ = self.attention(encoder_outputs, h[-1])\n",
    "            lstm_input = torch.cat((embeddings[:, t, :], attention_weighted_encoding), dim=1).unsqueeze(1)\n",
    "            output, (h, c) = self.lstm(lstm_input, (h, c))\n",
    "            output = self.fc(output.squeeze(1))\n",
    "            outputs.append(output)\n",
    "        \n",
    "        return torch.stack(outputs, dim=1)\n",
    "\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_heads, hidden_dim, num_layers, max_len=50):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        self.token_embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.pos_embed = nn.Embedding(max_len, embed_size)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=embed_size, nhead=num_heads, dim_feedforward=hidden_dim)\n",
    "        self.transformer = nn.TransformerDecoder(decoder_layer, num_layers)\n",
    "        self.fc_out = nn.Linear(embed_size, vocab_size)\n",
    "        self.scale = math.sqrt(embed_size)\n",
    "    \n",
    "    def forward(self, encoder_outputs, captions):\n",
    "        seq_len = captions.size(1)\n",
    "        positions = torch.arange(0, seq_len, device=captions.device).unsqueeze(0)\n",
    "        tgt = self.token_embed(captions) * self.scale + self.pos_embed(positions)\n",
    "        tgt = tgt.permute(1, 0, 2)\n",
    "        memory = encoder_outputs.permute(1, 0, 2)\n",
    "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(seq_len).to(captions.device)\n",
    "        output = self.transformer(tgt, memory, tgt_mask=tgt_mask)\n",
    "        output = output.permute(1, 0, 2)\n",
    "        return self.fc_out(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# الخلية 13: النموذج الرئيسي للتعليق على الصور\n",
    "# يجمع بين المشفر وفك التشفير (بأحد الأنواع الثلاثة)\n",
    "class CaptionModel(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, decoder_type=\"baseline\", transformer_params=None):\n",
    "        super(CaptionModel, self).__init__()\n",
    "        self.encoder = EncoderCNN(embed_size)\n",
    "        self.decoder_type = decoder_type\n",
    "        \n",
    "        if decoder_type == \"baseline\":\n",
    "            self.decoder = DecoderRNN(embed_size, hidden_size, vocab_size)\n",
    "        elif decoder_type == \"attention\":\n",
    "            self.decoder = DecoderWithLSTM(embed_size, hidden_size, vocab_size, encoder_dim=2048)\n",
    "        elif decoder_type == \"transformer\":\n",
    "            if transformer_params is None:\n",
    "                transformer_params = {\"num_heads\": 8, \"hidden_dim\": 2048, \"num_layers\": 2, \"max_len\": 50}\n",
    "            self.decoder = TransformerDecoder(vocab_size, embed_size, transformer_params[\"num_heads\"], \n",
    "                                             transformer_params[\"hidden_dim\"], transformer_params[\"num_layers\"], \n",
    "                                             transformer_params[\"max_len\"])\n",
    "        else:\n",
    "            raise ValueError(\"Unknown decoder type\")\n",
    "    \n",
    "    def forward(self, images, captions, lengths=None):\n",
    "        pooled, features_seq = self.encoder(images)\n",
    "        if self.decoder_type == \"baseline\":\n",
    "            return self.decoder(pooled, captions, lengths)\n",
    "        elif self.decoder_type == \"attention\":\n",
    "            return self.decoder(features_seq, captions)\n",
    "        elif self.decoder_type == \"transformer\":\n",
    "            return self.decoder(features_seq, captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# الخلية 14: إعداد النموذج، دالة الخسارة، والمحسن\n",
    "# تحديد المعلمات وتهيئة النموذج للتدريب\n",
    "EMBED_SIZE = 256\n",
    "HIDDEN_SIZE = 512\n",
    "NUM_EPOCHS = 5\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "vocab_size = len(vocab.itos)\n",
    "decoder_type = \"attention\"\n",
    "model = CaptionModel(EMBED_SIZE, HIDDEN_SIZE, vocab_size, decoder_type=decoder_type).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab.stoi[\"<PAD>\"])\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "080d3108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# الخلية 15: وظيفة حساب الـ accuracy\n",
    "# تقوم بحساب دقة النموذج بتجاهل كلمات الحشو (PAD)\n",
    "def calculate_accuracy(predictions, targets, vocab):\n",
    "    \"\"\"\n",
    "    حساب دقة النموذج\n",
    "    predictions: shape (batch_size * seq_len, vocab_size)\n",
    "    targets: shape (batch_size * seq_len)\n",
    "    \"\"\"\n",
    "    # الحصول على الكلمات المتوقعة\n",
    "    _, predicted_words = predictions.max(1)\n",
    "    \n",
    "    # تجاهل كلمات PAD في الحساب\n",
    "    non_pad_mask = targets != vocab.stoi[\"<PAD>\"]\n",
    "    \n",
    "    if non_pad_mask.sum().item() == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # حساب الدقة فقط للكلمات غير PAD\n",
    "    correct = (predicted_words[non_pad_mask] == targets[non_pad_mask]).sum().item()\n",
    "    total = non_pad_mask.sum().item()\n",
    "    \n",
    "    accuracy = correct / total * 100\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# الخلية 16: وظيفة التدريب مع حساب الـ accuracy\n",
    "# تقوم بتدريب النموذج وحساب الدقة في كل خطوة\n",
    "def train_model(model, loader, criterion, optimizer, num_epochs, device, vocab):\n",
    "    model.train()\n",
    "    total_step = len(loader)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "        epoch_accuracy = 0\n",
    "        total_samples = 0\n",
    "        \n",
    "        for i, (images, captions, lengths) in enumerate(loader):\n",
    "            images = images.to(device)\n",
    "            captions = captions.to(device)\n",
    "            batch_size = images.size(0)\n",
    "            \n",
    "            if model.decoder_type == \"baseline\":\n",
    "                targets = captions[:, 1:]\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(images, captions[:, :-1], lengths-1)\n",
    "                targets_packed = pack_padded_sequence(targets, lengths.cpu()-1, batch_first=True)[0]\n",
    "                loss = criterion(outputs, targets_packed)\n",
    "                \n",
    "                # حساب الaccuracy للـbatch\n",
    "                accuracy = calculate_accuracy(outputs, targets_packed, vocab)\n",
    "                \n",
    "            elif model.decoder_type == \"attention\":\n",
    "                inputs = captions[:, :-1]\n",
    "                targets = captions[:, 1:]\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(images, inputs)\n",
    "                outputs_reshaped = outputs.reshape(-1, outputs.size(2))\n",
    "                targets_reshaped = targets.reshape(-1)\n",
    "                loss = criterion(outputs_reshaped, targets_reshaped)\n",
    "                \n",
    "                # حساب الaccuracy للـbatch\n",
    "                accuracy = calculate_accuracy(outputs_reshaped, targets_reshaped, vocab)\n",
    "                \n",
    "            elif model.decoder_type == \"transformer\":\n",
    "                inputs = captions[:, :-1]\n",
    "                targets = captions[:, 1:]\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(images, inputs)\n",
    "                outputs_reshaped = outputs.reshape(-1, outputs.size(2))\n",
    "                targets_reshaped = targets.reshape(-1)\n",
    "                loss = criterion(outputs_reshaped, targets_reshaped)\n",
    "                \n",
    "                # حساب الaccuracy للـbatch\n",
    "                accuracy = calculate_accuracy(outputs_reshaped, targets_reshaped, vocab)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item() * batch_size\n",
    "            epoch_accuracy += accuracy * batch_size\n",
    "            total_samples += batch_size\n",
    "            \n",
    "            if (i+1) % 100 == 0:\n",
    "                print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{total_step}], Loss: {loss.item():.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "        \n",
    "        # حساب متوسط الـloss والـaccuracy للـepoch\n",
    "        avg_epoch_loss = epoch_loss / total_samples\n",
    "        avg_epoch_accuracy = epoch_accuracy / total_samples\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] - Average Loss: {avg_epoch_loss:.4f}, Average Accuracy: {avg_epoch_accuracy:.2f}%\")\n",
    "        \n",
    "        # حفظ النموذج بعد كل epoch\n",
    "        save_model(model, epoch+1)\n",
    "    \n",
    "    print(\"Training finished.\")\n",
    "    return model\n",
    "\n",
    "\n",
    "# الخلية 17: وظيفة حفظ النموذج\n",
    "# تحفظ النموذج والأوزان بعد كل epoch\n",
    "def save_model(model, epoch):\n",
    "    # حفظ الأوزان كملف .pth\n",
    "    torch.save(model.state_dict(), f'model_epoch_{epoch}.pth')\n",
    "    \n",
    "    # حفظ النموذج الكامل كملف .pkl\n",
    "    torch.save(model, f'model_complete_epoch_{epoch}.pkl')\n",
    "    \n",
    "    # حفظ معلومات التدريب\n",
    "    training_info = {\n",
    "        'epoch': epoch,\n",
    "        'vocab_size': len(vocab.itos),\n",
    "        'embed_size': EMBED_SIZE,\n",
    "        'hidden_size': HIDDEN_SIZE,\n",
    "        'decoder_type': decoder_type\n",
    "    }\n",
    "    torch.save(training_info, f'training_info_epoch_{epoch}.pth')\n",
    "    \n",
    "    print(f\"Model saved after epoch {epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2], Step [100/2000], Loss: 4.8017, Accuracy: 29.03%\n",
      "Epoch [1/2], Step [200/2000], Loss: 4.1667, Accuracy: 29.89%\n",
      "Epoch [1/2], Step [300/2000], Loss: 3.7918, Accuracy: 38.53%\n",
      "Epoch [1/2], Step [400/2000], Loss: 3.4692, Accuracy: 38.67%\n",
      "Epoch [1/2], Step [500/2000], Loss: 3.6239, Accuracy: 37.12%\n",
      "Epoch [1/2], Step [600/2000], Loss: 3.7009, Accuracy: 33.48%\n",
      "Epoch [1/2], Step [700/2000], Loss: 3.8100, Accuracy: 37.89%\n",
      "Epoch [1/2], Step [800/2000], Loss: 3.2850, Accuracy: 38.30%\n"
     ]
    }
   ],
   "source": [
    "# الخلية 18: بدء التدريب\n",
    "# هنا نبدأ عملية تدريب النموذج\n",
    "model = train_model(model, loader, criterion, optimizer, 2, device, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ead47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# الخلية 19: وظيفة توليد تعليقات باستخدام Beam Search\n",
    "# تستخدم خوارزمية Beam Search لتوليد أفضل التعليقات للصور\n",
    "def generate_caption_beam(model, image, vocab, device, beam_size=3, max_len=20):\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        image = image.unsqueeze(0).to(device)\n",
    "        _, encoder_outputs = model.encoder(image)\n",
    "        \n",
    "        sequences = [\n",
    "            (\n",
    "                [vocab.stoi[\"<SOS>\"]],\n",
    "                0.0,\n",
    "                torch.zeros(1, 1, model.decoder.lstm.hidden_size).to(device),\n",
    "                torch.zeros(1, 1, model.decoder.lstm.hidden_size).to(device)\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        for _ in range(max_len):\n",
    "            all_candidates = []\n",
    "            \n",
    "            for seq, score, h, c in sequences:\n",
    "                last_word = torch.tensor([seq[-1]]).to(device)\n",
    "                \n",
    "                if last_word.item() == vocab.stoi[\"<EOS>\"]:\n",
    "                    all_candidates.append((seq, score, h, c))\n",
    "                    continue\n",
    "                \n",
    "                embed = model.decoder.embedding(last_word).squeeze(1)\n",
    "                attn_output, _ = model.decoder.attention(encoder_outputs, h[-1])\n",
    "                lstm_input = torch.cat((embed, attn_output), dim=1).unsqueeze(1)\n",
    "                output, (h_new, c_new) = model.decoder.lstm(lstm_input, (h, c))\n",
    "                logits = model.decoder.fc(output.squeeze(1))\n",
    "                log_probs = torch.log_softmax(logits, dim=1)\n",
    "                \n",
    "                top_log_probs, top_indices = log_probs.topk(beam_size)\n",
    "                \n",
    "                for i in range(beam_size):\n",
    "                    candidate = (\n",
    "                        seq + [top_indices[0][i].item()],\n",
    "                        score + top_log_probs[0][i].item(),\n",
    "                        h_new,\n",
    "                        c_new\n",
    "                    )\n",
    "                    all_candidates.append(candidate)\n",
    "            \n",
    "            sequences = sorted(all_candidates, key=lambda x: x[1], reverse=True)[:beam_size]\n",
    "        \n",
    "        best_sequence = sequences[0][0]\n",
    "        \n",
    "        caption = [\n",
    "            vocab.itos[idx]\n",
    "            for idx in best_sequence\n",
    "            if idx not in (vocab.stoi[\"<SOS>\"], vocab.stoi[\"<EOS>\"], vocab.stoi[\"<PAD>\"])\n",
    "        ]\n",
    "    \n",
    "    return \" \".join(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783103b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# الخلية 20: اختبار النموذج بتوليد تعليق لصورة\n",
    "# نقوم باختبار النموذج على صورة معينة ومقارنة النتيجة بالتعليقات الحقيقية\n",
    "idx = 11\n",
    "image, captions = coco_train[idx]\n",
    "\n",
    "generated_caption = generate_caption_beam(\n",
    "    model=model,\n",
    "    image=image,\n",
    "    vocab=vocab,\n",
    "    device=device,\n",
    "    beam_size=3,\n",
    "    max_len=20\n",
    ")\n",
    "\n",
    "print(\"Generated Caption:\")\n",
    "print(generated_caption)\n",
    "\n",
    "print(\"\\nGround Truth Captions:\")\n",
    "for i, c in enumerate(captions):\n",
    "    print(f\"{i+1}. {c}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77347da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# الخلية 21: تحميل نموذج محفوظ\n",
    "# وظيفة لتحميل نموذج محفوظ مسبقاً للاستمرار في التدريب أو التقييم\n",
    "def load_model(epoch, device):\n",
    "    # تحميل الأوزان\n",
    "    model = CaptionModel(EMBED_SIZE, HIDDEN_SIZE, vocab_size, decoder_type=decoder_type).to(device)\n",
    "    model.load_state_dict(torch.load(f'model_epoch_{epoch}.pth'))\n",
    "    \n",
    "    # تحميل معلومات التدريب\n",
    "    training_info = torch.load(f'training_info_epoch_{epoch}.pth')\n",
    "    \n",
    "    model.eval()\n",
    "    return model, training_info\n",
    "\n",
    "\n",
    "# مثال لتحميل النموذج من epoch 1\n",
    "# loaded_model, info = load_model(1, device)\n",
    "# print(f\"Loaded model from epoch {info['epoch']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb58c2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# الخلية 22: تقييم النموذج على مجموعة اختبار\n",
    "# يمكن استخدام هذه الوظيفة لتقييم أداء النموذج على بيانات جديدة\n",
    "def evaluate_model(model, test_loader, device, vocab):\n",
    "    model.eval()\n",
    "    total_accuracy = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, captions, lengths in test_loader:\n",
    "            images = images.to(device)\n",
    "            captions = captions.to(device)\n",
    "            batch_size = images.size(0)\n",
    "            \n",
    "            if model.decoder_type == \"baseline\":\n",
    "                targets = captions[:, 1:]\n",
    "                outputs = model(images, captions[:, :-1], lengths-1)\n",
    "                targets_packed = pack_padded_sequence(targets, lengths.cpu()-1, batch_first=True)[0]\n",
    "                accuracy = calculate_accuracy(outputs, targets_packed, vocab)\n",
    "                \n",
    "            elif model.decoder_type == \"attention\" or model.decoder_type == \"transformer\":\n",
    "                inputs = captions[:, :-1]\n",
    "                targets = captions[:, 1:]\n",
    "                outputs = model(images, inputs)\n",
    "                outputs_reshaped = outputs.reshape(-1, outputs.size(2))\n",
    "                targets_reshaped = targets.reshape(-1)\n",
    "                accuracy = calculate_accuracy(outputs_reshaped, targets_reshaped, vocab)\n",
    "            \n",
    "            total_accuracy += accuracy * batch_size\n",
    "            total_samples += batch_size\n",
    "    \n",
    "    avg_accuracy = total_accuracy / total_samples\n",
    "    print(f\"Test Accuracy: {avg_accuracy:.2f}%\")\n",
    "    return avg_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9245217",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
